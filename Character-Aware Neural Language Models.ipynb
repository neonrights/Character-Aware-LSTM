{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character-Aware Neural Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A summary and demonstration by Nicholas Farn. contact: <nfarn@g.ucla.edu>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will describe and implement a word prediction model proposed in the paper <i>Character-Aware Neural Language Models</i> by Yoon Kim et al. At its simplest level, it is an amalgamation of a convolutional neural network, a highway network, and a long short term memory recurrent neural network. The CNN takes the characters of a given word as input, then combines its output with a highway network which is then fed into the LSTM. The LSTM then produces a word-level prediction. The model is trained on the Penn Treebank, as sample of which is imported below. A representation of the model's architecture can be viewed in <b>Figure 1</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from data_loader import Loader\n",
    "\n",
    "tf.set_random_seed(1337)\n",
    "data = Loader(batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](character-model.png \"Architecture\")\n",
    "<p style='text-align: center;'><b>Figure 1:</b> Example Architecture of Character-Aware Neural Network</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character-level Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from <b>Figure 1</b>, the base layers is a convolutional neural network. The cNN takes the characters in a word as input, this can be reprented as a vector $\\mathbf{w}_k$, the $k$-th word in a sequence, with character $c_{kj}$, the id of the $j$-th character in word $k$. Since each word has variable length, each word is padded to a uniform length equal to the length of the longest word. Each word also has a start and end character prepended and appended to it before padding, which aids accuracy of the model. Additionally each sequence taken to have a length of 35, since the LSTM is trained using truncated backpropagation up to 35 time-steps. This is discuseed in more detail later. Both of these changes are to increase ease during batch training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Longest word length: %d\" % data.max_word_len)\n",
    "print(\"Sequence length: %d\" % data.seq_len)\n",
    "\n",
    "char_inputs = tf.placeholder(tf.int32, [data.batch_size, data.seq_len, data.max_word_len])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each character is then embedded through the use of a matrix $\\mathbf{Q} \\in \\mathbb{R}^{d \\times \\vert \\mathcal{C} \\vert}$, where $\\mathcal{C}$ is the vocabulary of characters and $d$ is the dimension of the embeddings, in this case 15. Thus the input is converted into a matrix $\\mathbf{C}^k \\in \\mathbb{R}^{d \\times l}$ where $l$ is length of the longest word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embed_dim = 15\n",
    "print(\"Embedding dimension: %d\" % embed_dim)\n",
    "print(\"Character vocabulary size: %d\" % data.char_vocab_size)\n",
    "\n",
    "char_embeddings = tf.get_variable(\"char_embed\", [data.char_vocab_size, embed_dim])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernels of varying width are applied along word length, with a kernel with a width $i$ having a kernel $\\mathbf{H}_i \\in \\mathbb{R}^{d \\times i}$. The output convolution for a kernel $\\mathbf{H}_i$ is then placed through a tanh activation and then a max pool also along word length to learn the most significant filters. The resultant values are then combined into a single vector, resulting in a uniform output vector size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](character-rep.png \"character representations\")\n",
    "<p style='text-align: center;'><b>Figure 2:</b> Plot of character <i>n</i>-gram representations through PCA. The cNN is able to differentiate between prefixes (red) and suffixes (blue) with special attention to hyphenated (orange). All remaining words are in (grey).</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind varying kernel widths is to capture the most significant n-grams for a given word. Thus the cNN could potentially learn that the trigram \"foo\" is important in the word <b>foo</b>bar. The kernel widths are chosen to be of sizes 1 to 7 with filters of size 50 times width up to a max of 200 filters. The model's ability to differentiate prefixes, suffixes, and hyphenated morphemes can be seen in <b>Figure 2</b>. The specific equations are defined and implemented below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\mathbf{y}^k &= [y_1^k, \\dots, y_n^k] \\\\\n",
    "y_i^k &= \\max_j \\mathbf{f}^k [j] \\\\\n",
    "\\mathbf{f}^k [j] &= \\tanh(\\langle C^k[:, j:j + w_i - 1], \\mathbf{H_i} \\rangle + b_i) \\\\\n",
    "\\langle \\mathbf{A}, \\mathbf{B} \\rangle &= \\text{Tr}(\\mathbf{AB}^T)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create init functions\n",
    "weight_init = lambda shape : tf.Variable(tf.random_uniform(shape, minval=-0.05, maxval=0.05))\n",
    "bias_init = lambda shape : tf.Variable(tf.constant(0.1, shape=shape))\n",
    "conv_init = lambda x, W : tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='VALID')\n",
    "\n",
    "# set input and filter dimensions\n",
    "kernel_widths = np.arange(1,8)\n",
    "\n",
    "# set filters and biases\n",
    "cnn_kernels = [\n",
    "    weight_init([1, width, embed_dim, min(200, 50*width)]) for width in kernel_widths\n",
    "]\n",
    "cnn_biases = [\n",
    "    bias_init([min(200, 50*width)]) for width in kernel_widths\n",
    "]\n",
    "\n",
    "# combine max output into one tensor, reshape into array\n",
    "cnn_outputs = list()\n",
    "char_indices = tf.split(char_inputs, data.seq_len, 1)\n",
    "for i in xrange(data.seq_len):\n",
    "    # get individual word, embed characters\n",
    "    char_embed = tf.nn.embedding_lookup(char_embeddings, char_indices[i])\n",
    "    \n",
    "    # create convolutions, combine results to uniformly sized vector\n",
    "    layers = list()\n",
    "    for width, kernel, bias in zip(*[kernel_widths, cnn_kernels, cnn_biases]):\n",
    "        conv = tf.tanh(conv_init(char_embed, kernel) + bias)\n",
    "        pool = tf.nn.max_pool(conv, [1, 1, data.max_word_len - width + 1, 1], [1, 1, 1, 1], 'VALID')\n",
    "        layers.append(tf.squeeze(pool))\n",
    "    \n",
    "    cnn_outputs.append(tf.concat(layers, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Highway Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resultant output from the cNN could be fed directly into the LSTM, however instead it is run through a highway network. A highway network introduces an adaptive gate that can adaptively carry some input while throwing out others. The highway network is completely described below, where $\\circ$ represents element-wise multiplication and $\\mathbf{W}_H$ and $\\mathbf{W}_T$ are square matrices in order to give $\\mathbf{z}$ the same dimension as $\\mathbf{y}$. Furthermore, $\\mathbf{t}$ is described as a transform gate and $1 - \\mathbf{t}$ is known as the carry gate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\mathbf{z} &= \\mathbf{t} \\circ g(\\mathbf{W}_H y + \\mathbf{b}_H) + (1 - \\mathbf{t}) \\circ \\mathbf{y} \\\\\n",
    "\\mathbf{t} &= \\sigma(\\mathbf{W}_T \\mathbf{y} + \\mathbf{b}_T)\\\\\n",
    "g(x) &= \\max(0, x) \\\\\n",
    "\\sigma(x) &= \\frac{1}{1 + e^{-x}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A highway network is noted to improve the results compared to feeding the output directly into the LSTM. If the cNN can be seen as extracting the most significant n-grams characters in a word, a highway network can be seen as tossing out certain n-grams which are useless in the context of others. In the trained model, it is noted that a highway layer seems to encode semantic meaning, producing similar output for words that are very different character-wise but close semantically. The highway network is implemented below. Direct cNN input and highway input will be  compared later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hwy_inputs = cnn_outputs\n",
    "N = sum([min(200, 50*width) for width in kernel_widths])\n",
    "\n",
    "# initialize highway weights and biases\n",
    "weight_T = weight_init([N, N])\n",
    "weight_H = weight_init([N, N])\n",
    "bias_T = bias_init([N])\n",
    "bias_H = bias_init([N])\n",
    "\n",
    "# compute new output\n",
    "hwy_outputs = list()\n",
    "for hwy_input in hwy_inputs:\n",
    "    trans_gate = tf.sigmoid(tf.matmul(hwy_input, weight_T) + bias_T)\n",
    "    trans_output = tf.multiply(trans_gate, tf.nn.relu(tf.matmul(hwy_input, weight_H)) + bias_H)\n",
    "    carry_output = tf.multiply(1 - trans_gate, hwy_input)\n",
    "    hwy_outputs.append(trans_output + carry_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recurrent neural network is a simply 2 layer LSTM. The specific model is described by the following equations, where $\\sigma$ is a sigmoid function. Additionally, $\\mathbf{i}_t$, $\\mathbf{f}_t$, and $\\mathbf{o}_t$ are the <i>input</i>, <i>forget</i>, and <i>output</i> gates respectively at time-step $t$. $\\mathbf{h}_t$ and $\\mathbf{c}_t$ are the hidden and cell vectors and are zero-vectors when $t = 0$. The hidden and memory cells are chosen to have a dimension of 600. Dropout is also applied to these nodes to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\\begin{aligned}\n",
    "\\mathbf{i}_t &= \\sigma(\\mathbf{W}^i \\mathbf{x}_t + \\mathbf{U}^i \\mathbf{h}_{t-1} + \\mathbf{b}^i) \\\\\n",
    "\\mathbf{f}_t &= \\sigma(\\mathbf{W}^f \\mathbf{x}_t + \\mathbf{U}^f \\mathbf{h}_{t-1} + \\mathbf{b}^f) \\\\\n",
    "\\mathbf{i}_o &= \\sigma(\\mathbf{W}^o \\mathbf{x}_t + \\mathbf{U}^o \\mathbf{h}_{t-1} + \\mathbf{b}^o) \\\\\n",
    "\\mathbf{g}_t &= \\tanh(\\mathbf{W}^g \\mathbf{x}_t + \\mathbf{U}^g \\mathbf{h}_{t-1} + \\mathbf{b}^g) \\\\\n",
    "\\mathbf{c}_t &= \\mathbf{f}_t \\circ \\mathbf{c}_{t-1} + \\mathbf{i}_t \\circ \\mathbf{g}_t \\\\\n",
    "\\mathbf{h}_t &= \\mathbf{o}_t \\circ \\tanh(\\mathbf{c}_t)\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# implement both lstm cells, direct and highway input\n",
    "lstm_inputs = hwy_outputs\n",
    "lstm_dim = 600\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# add dropout to second layer for later\n",
    "lstm_cell1 = tf.nn.rnn_cell.BasicLSTMCell(lstm_dim)\n",
    "lstm_cell2 = tf.nn.rnn_cell.BasicLSTMCell(lstm_dim)\n",
    "lstm_cell2 = tf.nn.rnn_cell.DropoutWrapper(lstm_cell2, input_keep_prob=keep_prob, output_keep_prob=keep_prob)\n",
    "\n",
    "lstm_stacked_cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell1, lstm_cell2])\n",
    "init_state = lstm_stacked_cell.zero_state(data.batch_size, tf.float32)\n",
    "outputs, states = tf.contrib.rnn.static_rnn(lstm_stacked_cell, lstm_inputs, initial_state=init_state)\n",
    "\n",
    "lstm_outputs = list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output at time $t$ is achieved by taking the softmax after an affine transformation to the hidden output at time $t$, $\\mathbf{h}_t$. This creates a probability distribution over all possible words. The models is then rated using perplexity, the exponent of the average log-likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\Pr(w_{t+1} = j | w_{1:t}) &= \\frac{\\exp(\\mathbf{h}_t \\cdot \\mathbf{p}^j + q^j)}{\\sum_{j' \\in \\mathcal{V}} \\exp(\\mathbf{h}_t \\cdot \\mathbf{p}^{j'} + q^{j'})} \\\\\n",
    "NLL &= -\\sum_{t=1}^T \\log \\Pr(w_t | w_{1:t-1}) \\\\\n",
    "PPL &= \\exp(\\frac{NLL}{T})\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $\\mathbf{p}^j$ is the $j$-th column of $\\mathbf{P} \\in \\mathbb{R}^{m \\times \\vert \\mathcal{V} \\vert}$, an output embedding matrix and $q^j$ is a bias term. Here $\\mathcal{V}$ is simply our vocabulary of words. An lstm taking input from the highway network is both implemented below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = 0\n",
    "predictions = list()\n",
    "\n",
    "true_outputs = tf.placeholder(tf.int32, [data.batch_size, data.seq_len])\n",
    "weight_P = weight_init([lstm_dim, data.word_vocab_size])\n",
    "bias_Q = bias_init([data.word_vocab_size])\n",
    "\n",
    "list_true_outputs = tf.split(true_outputs, data.seq_len, 1)\n",
    "for hidden, true_output in zip(outputs, list_true_outputs):\n",
    "    predicted = tf.matmul(hidden, weight_P) + bias_Q\n",
    "    predictions.append(predicted)\n",
    "    loss += tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=tf.squeeze(true_output),\n",
    "        logits=predicted\n",
    "    )\n",
    "\n",
    "loss = tf.reduce_mean(loss) / data.seq_len\n",
    "perplexity = tf.exp(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models are trained through truncated backpropagation up to 35 time steps. The learning rate is initially set to 1.0 and is halved if the perplexity is not decreased by 1.0 per training epoch out of 30 epochs. In addition, the model is regularized using a dropout rate of 0.5 for the input to hidden and hidden to output layers with the exception of input from the highway layer. Finally, the gradient is renormalized so that its $L_2$ norm is less than or equal to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(max_to_keep=35)\n",
    "with tf.Session() as sess:\n",
    "    epochs = 30\n",
    "    train_PPLs = list()\n",
    "    valid_PPLs = list()\n",
    "    final_PPLs = (0, 0, 0)\n",
    "    max_gradient_norm = 5\n",
    "    rate = 1.0\n",
    "    learning_rate = tf.Variable(rate, trainable=False)\n",
    "    if not os.path.isdir(\"tmp\"):\n",
    "        os.mkdir(\"tmp\")\n",
    "    \n",
    "    # normalize gradients down to max\n",
    "    trainables = tf.trainable_variables()\n",
    "    gradients = list()\n",
    "    for grad in tf.gradients(loss, trainables):\n",
    "        if grad is not None:\n",
    "            gradients.append(tf.clip_by_norm(grad, max_gradient_norm))\n",
    "        else:\n",
    "            grads.append(grad)\n",
    "\n",
    "    # create optimizer\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, trainables),\n",
    "        global_step=global_step\n",
    "    )\n",
    "\n",
    "    # initialize all variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in xrange(epochs):\n",
    "        # calculate validation loss, 1:validation set\n",
    "        valid_loss = 0\n",
    "        for j in xrange(data.batch_count[1]):\n",
    "            x, y = data.next_batch(1)\n",
    "            \n",
    "            if x is None or y is None:\n",
    "                break\n",
    "            \n",
    "            feed_dict = {char_inputs: x, true_outputs: y, keep_prob: 1.0}\n",
    "            valid_loss += sess.run(loss, feed_dict=feed_dict)\n",
    "\n",
    "        valid_loss /= data.batch_count[1]\n",
    "        valid_PPLs.append(np.exp(valid_loss))\n",
    "        data.next_epoch(1)\n",
    "        \n",
    "        print(\"Epoch: %d, Valid PPL: %2.6f, Learning rate: %1.6f\" %\n",
    "             (i, valid_PPLs[-1], rate))\n",
    "        \n",
    "        if i > 0 and (valid_PPLs[-2] - valid_PPLs[-1]) < 1:\n",
    "            rate *= 0.5\n",
    "            learning_rate.assign(rate).eval()\n",
    "        \n",
    "        if rate < 1e-5:\n",
    "            break # done with training\n",
    "        \n",
    "        try:\n",
    "            saver = tf.train.import_meta_graph(\"tmp/model-e%d.ckpt.meta\" % i)\n",
    "            saver.restore(sess, \"tmp/model-e%d.ckpt\" % i)\n",
    "        except (tf.errors.NotFoundError, IOError) as e:\n",
    "            # start training 0:training set\n",
    "            train_loss = 0\n",
    "            for j in xrange(data.batch_count[0]):\n",
    "                x, y = data.next_batch(0)\n",
    "\n",
    "                if x is None or y is None:\n",
    "                    break\n",
    "\n",
    "                feed_dict = {char_inputs: x, true_outputs: y, keep_prob: 0.5}\n",
    "                _, batch_loss = sess.run(\n",
    "                    [optimizer, loss], feed_dict=feed_dict\n",
    "                )\n",
    "\n",
    "                train_loss += batch_loss\n",
    "\n",
    "                if (j+1) % 50 == 0:\n",
    "                    print(\"Epoch: %d, batch: %d/%d, loss: %2.6f\" %\n",
    "                          (i+1, j+1, data.batch_count[0], batch_loss))\n",
    "\n",
    "            train_loss /= data.batch_count[0]\n",
    "            train_PPLs.append(np.exp(train_loss))\n",
    "            data.next_epoch(0)\n",
    "\n",
    "            saver.save(sess, \"tmp/model-e%d.ckpt\" % i)\n",
    "    \n",
    "    saver.save(sess, \"tmp/final-model.ckpt\")\n",
    "    \n",
    "    # calculate final perplexities\n",
    "    for i in xrange(2):\n",
    "        for j in xrange(data.batch_count[i]):\n",
    "            x, y = data.next_batch(0)\n",
    "\n",
    "            if x is None or y is None:\n",
    "                break\n",
    "\n",
    "            feed_dict = {char_inputs: x, true_outputs: y, keep_prob: 1.0}\n",
    "            final_PPLs[i] += sess.run(loss, feed_dict=feed_dict)\n",
    "\n",
    "        final_PPLs[i] /= data.batch_count[i]\n",
    "        final_PPLs[i] = np.exp(final_PPLs[i])\n",
    "    \n",
    "print(\"Final train perplexity: %2.6f, Final valid perplexity: %2.6f\" %\n",
    "     (final_PPLs[0]), final_PPLs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kim et al was able to achieve a perplexity of 78.9 on the english penn treebank. It is also noted that the model handles mispellings and unseen words well, mapping them closely to related words within its vocabulary. For comparison, here is the performance of the same test set on the trained model with the best validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test set perplexity 2:test set\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.import_meta_graph(\"/tmp/final-model.ckpt.meta\" % i)\n",
    "    saver.restore(sess, \"/tmp/final-model.ckpt\")\n",
    "    for i in xrange(data.batch_count[2]):\n",
    "        x, y = data.next_batch(2)\n",
    "\n",
    "        if x is None or y is None:\n",
    "            break\n",
    "\n",
    "        feed_dict = {char_inputs: x, true_outputs: y, keep_prob: 1.0}\n",
    "        final_PPLs[2] += sess.run(loss, feed_dict=feed_dict)\n",
    "        \n",
    "    final_PPLs[2] /= data.batch_count[2]\n",
    "    final_PPLs[2] = np.exp(valid_loss)\n",
    "    data.next_epoch(2)\n",
    "\n",
    "print(\"Final Model Test Perplexity: %2.6f\" % final_PPLs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A character-aware neural network is a very novel idea and seems to perform well in making character to word predictions. Personally, I had difficulty training the model successfully according to the method outlined by Kim et al. A cursory glance at the computed validation perplexity compared to the training perplexity seems to imply that the model is overfitting its parameters. Assuming the model is sound, that would in turn imply that a dropout with a rate of 0.5 is not sufficient regularization. A good followup to this character to word model would be to design and train a character to chararacter model since the current model has a fixed size vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References and Source Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/1508.06615.pdf\n",
    "\n",
    "https://github.com/neonrights/Character-Aware-LSTM"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
