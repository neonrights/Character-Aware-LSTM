{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character-Aware Neural Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A summary and demonstration by Nicholas Farn. contact: <nfarn@g.ucla.edu>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will describe a \"Character-Aware Neural Language Model\". At its simplest level, it is an amalgamation of a convolutional neural network, a highway network, and a long short term memory recurrent neural network. The CNN takes the characters of a given word as input, then combines its output with a highway network which is then fed into the LSTM. The LSTM then produces a word-level prediction. The model is trained on the Penn Treebank, as sample of which is imported below. A representation of the model's architecture can be viewed in <b>Figure 1</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished processing data sets. batch size: 100\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from data_loader import Loader\n",
    "\n",
    "# import character and word representations\n",
    "data = Loader(batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](character-model.png \"Architecture\")\n",
    "<p style='text-align: center;'><b>Figure 1:</b> Example Architecture of Character-Aware Neural Network</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character-level Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from <b>Figure 1</b>, the base layers is a convolutional neural network. The cNN takes the characters in a word as input, this can be reprented as a vector $\\mathbf{w}_k$, the $k$-th word in a sequence, with character $c_{kj}$, the id of the $j$-th character in word $k$. Since each word has variable length, each word is padded to a uniform length equal to the length of the longest word. Each word also has a start and end character prepended and appended to it before padding, which aids accuracy of the model. Additionally each sequence taken to have a length of 35, since the LSTM is trained using truncated backpropagation up to 35 time-steps. This is discuseed in more detail later. Both of these changes are to increase ease during batch training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest word length: 21\n",
      "Sequence length: 35\n"
     ]
    }
   ],
   "source": [
    "print(\"Longest word length: %d\" % data.max_word_len)\n",
    "print(\"Sequence length: %d\" % data.seq_len)\n",
    "\n",
    "char_inputs = tf.placeholder(tf.int32, [data.batch_size, data.seq_len, data.max_word_len])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each character is then embedded through the use of a matrix $\\mathbf{Q} \\in \\mathbb{R}^{d \\times \\vert \\mathcal{C} \\vert}$, where $\\mathcal{C}$ is the vocabulary of characters and $d$ is the dimension of the embeddings, in this case 15. Thus the input is converted into a matrix $\\mathbf{C}^k \\in \\mathbb{R}^{d \\times l}$ where $l$ is length of the longest word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimension: 15\n",
      "Character vocabulary size: 50\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 15\n",
    "print(\"Embedding dimension: %d\" % embed_dim)\n",
    "print(\"Character vocabulary size: %d\" % data.char_vocab_size)\n",
    "\n",
    "char_embeddings = tf.get_variable(\"char_embed\", [data.char_vocab_size, embed_dim])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernels of varying width are applied along word length, with a kernel with a width $i$ having a kernel $\\mathbf{H}_i \\in \\mathbb{R}^{d \\times i}$. The output convolution for a kernel $\\mathbf{H}_i$ is then placed through a tanh activation and then a max pool also along word length to learn the most significant filters. The resultant values are then combined into a single vector, resulting in a uniform output vector size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](character-rep.png \"character representations\")\n",
    "<p style='text-align: center;'><b>Figure 2:</b> Plot of character <i>n</i>-gram representations through PCA. The cNN is able to differentiate between prefixes (red) and suffixes (blue) with special attention to hyphenated (orange). All remaining words are in (grey).</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind varying kernel widths is to capture the most significant n-grams for a given word. Thus the cNN could potentially learn that the trigram \"foo\" is important in the word <b>foo</b>bar. The kernel widths are chosen to be of sizes 1 to 7 with filters of size 50 times width up to a max of 200 filters. The model's ability to differentiate prefixes, suffixes, and hyphenated morphemes can be seen in <b>Figure 2</b>. The specific equations are defined and implemented below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\mathbf{y}^k &= [y_1^k, \\dots, y_n^k] \\\\\n",
    "y_i^k &= \\max_j \\mathbf{f}^k [j] \\\\\n",
    "\\mathbf{f}^k [j] &= \\tanh(\\langle C^k[:, j:j + w_i - 1], \\mathbf{H_i} \\rangle + b_i) \\\\\n",
    "\\langle \\mathbf{A}, \\mathbf{B} \\rangle &= \\text{Tr}(\\mathbf{AB}^T)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create init functions\n",
    "weight_init = lambda shape : tf.Variable(tf.truncated_normal(shape, stddev=0.1))\n",
    "bias_init = lambda shape : tf.Variable(tf.constant(0.1, shape=shape))\n",
    "conv_init = lambda x, W : tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='VALID')\n",
    "\n",
    "# set input and filter dimensions\n",
    "kernel_widths = np.arange(1,8)\n",
    "\n",
    "# set filters and biases\n",
    "cnn_kernels = [\n",
    "    weight_init([1, width, embed_dim, min(200, 50*width)]) for width in kernel_widths\n",
    "]\n",
    "cnn_biases = [\n",
    "    bias_init([min(200, 50*width)]) for width in kernel_widths\n",
    "]\n",
    "\n",
    "# combine max output into one tensor, reshape into array\n",
    "cnn_outputs = list()\n",
    "char_indices = tf.split(char_inputs, data.seq_len, 1)\n",
    "for i in xrange(data.seq_len):\n",
    "    # get individual word, embed characters\n",
    "    char_embed = tf.nn.embedding_lookup(char_embeddings, char_indices[i])\n",
    "    \n",
    "    # create convolutions, combine results to uniformly sized vector\n",
    "    layers = list()\n",
    "    for width, kernel, bias in zip(*[kernel_widths, cnn_kernels, cnn_biases]):\n",
    "        conv = tf.tanh(conv_init(char_embed, kernel) + bias)\n",
    "        pool = tf.nn.max_pool(conv, [1, 1, data.max_word_len - width + 1, 1], [1, 1, 1, 1], 'VALID')\n",
    "        layers.append(tf.squeeze(pool))\n",
    "    \n",
    "    cnn_outputs.append(tf.concat(layers, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Highway Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resultant output from the cNN could be fed directly into the LSTM, however instead it is run through a highway network. A highway network introduces an adaptive gate that can adaptively carry some input while throwing out others. The highway network is completely described below, where $\\circ$ represents element-wise multiplication and $\\mathbf{W}_H$ and $\\mathbf{W}_T$ are square matrices in order to give $\\mathbf{z}$ the same dimension as $\\mathbf{y}$. Furthermore, $\\mathbf{t}$ is described as a transform gate and $1 - \\mathbf{t}$ is known as the carry gate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\mathbf{z} &= \\mathbf{t} \\circ g(\\mathbf{W}_H y + \\mathbf{b}_H) + (1 - \\mathbf{t}) \\circ \\mathbf{y} \\\\\n",
    "\\mathbf{t} &= \\sigma(\\mathbf{W}_T \\mathbf{y} + \\mathbf{b}_T)\\\\\n",
    "g(x) &= \\max(0, x) \\\\\n",
    "\\sigma(x) &= \\frac{1}{1 + e^{-x}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A highway network is noted to improve the results compared to feeding the output directly into the LSTM. If the cNN can be seen as extracting the most significant n-grams characters in a word, a highway network can be seen as tossing out certain n-grams which are useless in the context of others. In the trained model, it is noted that a highway layer seems to encode semantic meaning, producing similar output for words that are very different character-wise but close semantically. The highway network is implemented below. Direct cNN input and highway input will be  compared later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hwy_inputs = cnn_outputs\n",
    "N = sum([min(200, 50*width) for width in kernel_widths])\n",
    "\n",
    "# initialize highway weights and biases\n",
    "weight_T = weight_init([N, N])\n",
    "weight_H = weight_init([N, N])\n",
    "bias_T = bias_init([N])\n",
    "bias_H = bias_init([N])\n",
    "\n",
    "# compute new output\n",
    "hwy_outputs = list()\n",
    "for hwy_input in hwy_inputs:\n",
    "    trans_gate = tf.sigmoid(tf.matmul(hwy_input, weight_T) + bias_T)\n",
    "    trans_output = tf.multiply(trans_gate, tf.nn.relu(tf.matmul(hwy_input, weight_H)) + bias_H)\n",
    "    carry_output = tf.multiply(1 - trans_gate, hwy_input)\n",
    "    hwy_outputs.append(trans_output + carry_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recurrent neural network is a simply 2 layer LSTM. The specific model is described by the following equations, where $\\sigma$ is a sigmoid function. Additionally, $\\mathbf{i}_t$, $\\mathbf{f}_t$, and $\\mathbf{o}_t$ are the <i>input</i>, <i>forget</i>, and <i>output</i> gates respectively at time-step $t$. $\\mathbf{h}_t$ and $\\mathbf{c}_t$ are the hidden and cell vectors and are zero-vectors when $t = 0$. The hidden and memory cells are chosen to have a dimension of 600."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\\begin{aligned}\n",
    "\\mathbf{i}_t &= \\sigma(\\mathbf{W}^i \\mathbf{x}_t + \\mathbf{U}^i \\mathbf{h}_{t-1} + \\mathbf{b}^i) \\\\\n",
    "\\mathbf{f}_t &= \\sigma(\\mathbf{W}^f \\mathbf{x}_t + \\mathbf{U}^f \\mathbf{h}_{t-1} + \\mathbf{b}^f) \\\\\n",
    "\\mathbf{i}_o &= \\sigma(\\mathbf{W}^o \\mathbf{x}_t + \\mathbf{U}^o \\mathbf{h}_{t-1} + \\mathbf{b}^o) \\\\\n",
    "\\mathbf{g}_t &= \\tanh(\\mathbf{W}^g \\mathbf{x}_t + \\mathbf{U}^g \\mathbf{h}_{t-1} + \\mathbf{b}^g) \\\\\n",
    "\\mathbf{c}_t &= \\mathbf{f}_t \\circ \\mathbf{c}_{t-1} + \\mathbf{i}_t \\circ \\mathbf{g}_t \\\\\n",
    "\\mathbf{h}_t &= \\mathbf{o}_t \\circ \\tanh(\\mathbf{c}_t)\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# implement both lstm cells, direct and highway input\n",
    "lstm_inputs = hwy_outputs\n",
    "lstm_dim = 600\n",
    "lstm_layers = 2\n",
    "\n",
    "lstm_cells = [tf.contrib.rnn.BasicLSTMCell(lstm_dim) for _ in range(lstm_layers)]\n",
    "lstm_stacked_cell = tf.nn.rnn_cell.MultiRNNCell(lstm_cells)\n",
    "init_state = lstm_stacked_cell.zero_state(data.batch_size, tf.float32)\n",
    "outputs, states = tf.contrib.rnn.static_rnn(lstm_stacked_cell, lstm_inputs, initial_state=init_state)\n",
    "\n",
    "lstm_outputs = list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output at time $t$ is achieved by taking the softmax after an affine transformation to the hidden output at time $t$, $\\mathbf{h}_t$. This creates a probability distribution over all possible words. The models is then rated using perplexity, the exponent of the average log-likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\Pr(w_{t+1} = j | w_{1:t}) &= \\frac{\\exp(\\mathbf{h}_t \\cdot \\mathbf{p}^j + q^j)}{\\sum_{j' \\in \\mathcal{V}} \\exp(\\mathbf{h}_t \\cdot \\mathbf{p}^{j'} + q^{j'})} \\\\\n",
    "NLL &= -\\sum_{t=1}^T \\log \\Pr(w_t | w_{1:t-1}) \\\\\n",
    "PPL &= \\exp(\\frac{NLL}{T})\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $\\mathbf{p}^j$ is the $j$-th column of $\\mathbf{P} \\in \\mathbb{R}^{m \\times \\vert \\mathcal{V} \\vert}$, an output embedding matrix and $q^j$ is a bias term. Here $\\mathcal{V}$ is simply our vocabulary of words. An lstm taking input from the highway network is both implemented below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = 0\n",
    "drop_prob = 0.5\n",
    "\n",
    "true_outputs = tf.placeholder(tf.int32, [data.batch_size, data.seq_len])\n",
    "weight_P = weight_init([lstm_dim, data.word_vocab_size])\n",
    "bias_Q = bias_init([data.word_vocab_size])\n",
    "\n",
    "list_true_outputs = tf.split(true_outputs, data.seq_len, 1)\n",
    "for hidden, true_output in zip(outputs, list_true_outputs):\n",
    "    hidden = tf.nn.dropout(hidden, drop_prob)\n",
    "    predicted = tf.matmul(hidden, weight_P) + bias_Q\n",
    "    loss += tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=tf.squeeze(true_output),\n",
    "        logits=predicted\n",
    "    )\n",
    "\n",
    "loss = tf.reduce_mean(loss) / data.seq_len\n",
    "perplexity = tf.exp(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models are trained through truncated backpropagation up to 35 time steps. The learning rate is initially set to 1.0 and is halved if the perplexity is not decreased by 1.0 per training epoch. In addition, the model is regularized using a dropout rate of 0.5 and the gradient is renormalized so that its $L_2$ norm is less than or equal to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, batch: 0/253, loss: 9.278312\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    epochs = 50\n",
    "    train_PPLs = list()\n",
    "    valid_PPLs = list()\n",
    "    max_gradient_norm = 5\n",
    "    rate = 1.0\n",
    "    learning_rate = tf.Variable(rate, trainable=False)\n",
    "    \n",
    "    # normalize gradients down to max\n",
    "    trainables = tf.trainable_variables()\n",
    "    gradients = list()\n",
    "    for grad in tf.gradients(loss, trainables):\n",
    "        if grad is not None:\n",
    "            gradients.append(tf.clip_by_norm(grad, max_gradient_norm))\n",
    "        else:\n",
    "            grads.append(grad)\n",
    "\n",
    "    # create optimizer\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, trainables),\n",
    "        global_step=global_step\n",
    "    )\n",
    "\n",
    "    # initialize all variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # start training 0:training set\n",
    "    for i in xrange(epochs):\n",
    "        train_loss = 0\n",
    "        for j in xrange(data.batch_count[0]):\n",
    "            x, y = data.next_batch(0)\n",
    "            \n",
    "            if x is None or y is None:\n",
    "                break\n",
    "            \n",
    "            feed_dict = {char_inputs: x, true_outputs: y}\n",
    "            _, batch_loss, step = sess.run(\n",
    "                [optimizer, loss, global_step], feed_dict=feed_dict\n",
    "            )\n",
    "            \n",
    "            train_loss += batch_loss\n",
    "            \n",
    "            if j % 50 == 0:\n",
    "                print(\"Epoch: %d, batch: %d/%d, loss: %2.6f\" %\n",
    "                      (i+1, j+1, data.batch_count[0], batch_loss))\n",
    "        \n",
    "        train_loss /= data.batch_count[0]\n",
    "        train_PPLs.append(tf.exp(train_loss))\n",
    "        data.next_epoch(0)\n",
    "        \n",
    "        # calculate validation loss, 1:validation set\n",
    "        valid_loss = 0\n",
    "        for j in xrange(data.batch_count[1]):\n",
    "            x, y = data.next_batch(1)\n",
    "            \n",
    "            if x is None or y is None:\n",
    "                break\n",
    "            \n",
    "            feed_dict = {char_inputs: x, true_outputs: y}\n",
    "            batch_loss = sess.run(loss, feed_dict=feed_dict)\n",
    "            valid_loss += batch_loss\n",
    "        \n",
    "        valid_loss /= data.batch_count[1]\n",
    "        valid_PPLs.append(tf.exp(train_loss))\n",
    "        data.next_epoch(1)\n",
    "        \n",
    "        print(\"Epoch: %d, Train PPL: %2.6f, Valid PPL: %2.6f, Learn rate: %f\" %\n",
    "             (i+1, train_PPLs[-1], valid_PPLs[-1], rate))\n",
    "        \n",
    "        if i > 0 and (train_PPLs[-2] - train_PPLs[-1]) < 1:\n",
    "            rate *= 0.5\n",
    "            learning_rate.assign(rate).eval()\n",
    "        \n",
    "        if rate < 1e-5:\n",
    "            break # done with training\n",
    "\n",
    "print(\"Final train perplexity: %2.6f, Final valid perplexity: %2.6f\" %\n",
    "     (train_PPLs[-1]), valid_PPLs[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the performance of the model on the validation and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate test set perplexity\n",
    "# run model with starter string to produce random sentence\n",
    "start1 = \"Never have I ever \"\n",
    "start2 = \"The meaning of life is \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final thoughts, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/1508.06615.pdf\n",
    "\n",
    "https://github.com/carpedm20/lstm-char-cnn-tensorflow\n",
    "\n",
    "https://github.com/neonrights/Character-Aware-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
