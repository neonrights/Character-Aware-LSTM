{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character-Aware Neural Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A summary and demonstration by Nicholas Farn. contact: <nfarn@g.ucla.edu>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will describe a \"Character-Aware Neural Language Model\". At its simplest level, it is an amalgamation of a convolutional neural network, a highway network, and a long short term memory recurrent neural network. The CNN takes the characters of a given word as input, then combines its output with a highway network which is then fed into the LSTM. The LSTM then produces a word-level prediction. The model is trained on the Penn Treebank, as sample of which is imported below. A representation of the model's architecture can be viewed in <b>Figure 1</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# import character and word representations\n",
    "batch_size = 100\n",
    "char_table = json.loads(open('data/chars', 'r').read())\n",
    "word_table = json.loads(open('data/words', 'r').read())\n",
    "\n",
    "# import training, testing, and validation sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](character-model.png \"Architecture\")\n",
    "<p style='text-align: center;'><b>Figure 1:</b> Example Architecture of Character-Aware Neural Network</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character-level Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from <b>Figure 1</b>, the base layers is a convolutional neural network. The cNN takes the characters in a word as input, this can be reprented as a vector $\\mathbf{w}_k$, the $k$-th word in a sequence, with character $c_{kj}$, the id of the $j$-th character in word $k$. Since each word has variable length, each word is padded to a uniform length equal to the length of the longest word. Each word also has a start and end character prepended and appended to it before padding, which aids accuracy of the model. Additionally each sequence is padded to a length of 35, since the LSTM is trained using truncated backpropagation up to 35 time-steps. This is discuseed in more detail later. Both of these changes are to increase ease during batch training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_word_len = len(max(word_table.keys(), key=len))\n",
    "seq_len = 35\n",
    "\n",
    "char_input = tf.placeholder(tf.int32, [batch_size, seq_len, max_word_len])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each character is then embedded through the use of a matrix $\\mathbf{Q} \\in \\mathbb{R}^{d \\times \\vert \\mathcal{C} \\vert}$, where $\\mathcal{C}$ is the vocabulary of characters and $d$ is the dimension of the embeddings, in this case 15. Thus the input is converted into a matrix $\\mathbf{C}^k \\in \\mathbb{R}^{d \\times l}$ where $l$ is length of the longest word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "char_vocab_size = len(char_table)\n",
    "embed_dim = 15\n",
    "\n",
    "char_embeddings = tf.get_variable(\"char_embed\", [char_vocab_size, embed_dim])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernels of varying width are applied along word length, with a kernel with a width $i$ having a kernel $\\mathbf{H}_i \\in \\mathbb{R}^{d \\times i}$. The output convolution for a kernel $\\mathbf{H}_i$ is then placed through a tanh activation and then a max pool also along word length to learn the most significant filters. The resultant values are then combined into a single vector, resulting in a uniform output vector size.\n",
    "\n",
    "The idea behind varying kernel widths is to capture the most significant n-grams for a given word. Thus the cNN could potentially learn that the trigram \"foo\" is important in the word <b>foo</b>bar. The kernel widths are chosen to be of sizes 1 to 7 with filters of size 50 times width up to a max of 200 filters. The specific equations are defined and implemented below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\mathbf{y}^k &= [y_1^k, \\dots, y_n^k] \\\\\n",
    "y_i^k &= \\max_j \\mathbf{f}^k [j] \\\\\n",
    "\\mathbf{f}^k [j] &= \\tanh(\\langle C^k[:, j:j + w_i - 1], \\mathbf{H_i} \\rangle + b_i) \\\\\n",
    "\\langle \\mathbf{A}, \\mathbf{B} \\rangle &= \\text{Tr}(\\mathbf{AB}^T)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create init functions\n",
    "weight_init = lambda shape : tf.Variable(tf.truncated_normal(shape, stddev=0.1))\n",
    "bias_init = lambda shape : tf.Variable(tf.constant(0.1, shape=shape))\n",
    "conv_init = lambda x, W : tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='VALID')\n",
    "\n",
    "# set input and filter dimensions\n",
    "kernel_widths = np.arange(1,8)\n",
    "\n",
    "# set filters and biases\n",
    "cnn_kernels = [\n",
    "    weight_init([1, width, embed_dim, min(200, 50*width)]) for width in kernel_widths\n",
    "]\n",
    "cnn_biases = [\n",
    "    bias_init([min(200, 50*width)]) for width in kernel_widths\n",
    "]\n",
    "\n",
    "# combine max output into one tensor, reshape into array\n",
    "cnn_outputs = list()\n",
    "char_indices = tf.split(char_input, seq_len, 1)\n",
    "for i in xrange(seq_len):\n",
    "    # get individual word, embed characters\n",
    "    char_embed = tf.nn.embedding_lookup(char_embeddings, char_indices[i])\n",
    "    \n",
    "    # create convolutions, combine results to uniformly sized vector\n",
    "    layers = list()\n",
    "    for width, kernel, bias in zip(*[kernel_widths, cnn_kernels, cnn_biases]):\n",
    "        conv = tf.tanh(conv_init(char_embed, kernel) + bias)\n",
    "        pool = tf.nn.max_pool(conv, [1, 1, max_word_len - width + 1, 1], [1, 1, 1, 1], 'VALID')\n",
    "        layers.append(tf.squeeze(pool))\n",
    "    \n",
    "    cnn_outputs.append(tf.concat(layers, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Highway Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resultant output from the cNN could be fed directly into the LSTM, however instead it is run through a highway network. A highway network introduces an adaptive gate that can adaptively carry some input while throwing out others. The highway network is completely described below, where $\\circ$ represents element-wise multiplication and $\\mathbf{W}_H$ and $\\mathbf{W}_T$ are square matrices in order to give $\\mathbf{z}$ the same dimension as $\\mathbf{y}$. Furthermore, $\\mathbf{t}$ is described as a transform gate and $1 - \\mathbf{t}$ is known as the carry gate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\mathbf{z} &= \\mathbf{t} \\circ g(\\mathbf{W}_H y + \\mathbf{b}_H) + (1 - \\mathbf{t}) \\circ \\mathbf{y} \\\\\n",
    "\\mathbf{t} &= \\sigma(\\mathbf{W}_T \\mathbf{y} + \\mathbf{b}_T)\\\\\n",
    "g(x) &= \\max(0, x) \\\\\n",
    "\\sigma(x) &= \\frac{1}{1 + e^{-x}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A highway network is noted to improve the results compared to feeding the output directly into the LSTM. If the cNN can be seen as extracting the most significant n-grams characters in a word, a highway network can be seen as tossing out certain n-grams which are useless in the context of others. The highway network is implemented below. Direct cNN input and highway input will be  compared later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hwy_inputs = cnn_outputs\n",
    "N = sum([min(200, 50*width) for width in kernel_widths])\n",
    "\n",
    "# initialize highway weights and biases\n",
    "weight_T = weight_init([N, N])\n",
    "weight_H = weight_init([N, N])\n",
    "bias_T = bias_init([N])\n",
    "bias_H = bias_init([N])\n",
    "\n",
    "# compute new output\n",
    "hwy_outputs = list()\n",
    "for hwy_input in hwy_inputs:\n",
    "    trans_gate = tf.sigmoid(tf.matmul(hwy_input, weight_T) + bias_T)\n",
    "    trans_output = tf.multiply(trans_gate, tf.nn.relu(tf.matmul(hwy_input, weight_H)) + bias_H)\n",
    "    carry_output = tf.multiply(1 - trans_gate, hwy_input)\n",
    "    hwy_outputs.append(trans_output + carry_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recurrent neural network is a simply 2 layer LSTM. The specific model is described by the following equations, where $\\sigma$ is a sigmoid function. Additionally, $\\mathbf{i}_t$, $\\mathbf{f}_t$, and $\\mathbf{o}_t$ are the <i>input</i>, <i>forget</i>, and <i>output</i> gates respectively at time-step $t$. $\\mathbf{h}_t$ and $\\mathbf{c}_t$ are the hidden and cell vectors and are zero-vectors when $t = 0$. The memory cell is chosen to have a dimension of 600."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\\begin{aligned}\n",
    "\\mathbf{i}_t &= \\sigma(\\mathbf{W}^i \\mathbf{x}_t + \\mathbf{U}^i \\mathbf{h}_{t-1} + \\mathbf{b}^i) \\\\\n",
    "\\mathbf{f}_t &= \\sigma(\\mathbf{W}^f \\mathbf{x}_t + \\mathbf{U}^f \\mathbf{h}_{t-1} + \\mathbf{b}^f) \\\\\n",
    "\\mathbf{i}_o &= \\sigma(\\mathbf{W}^o \\mathbf{x}_t + \\mathbf{U}^o \\mathbf{h}_{t-1} + \\mathbf{b}^o) \\\\\n",
    "\\mathbf{g}_t &= \\tanh(\\mathbf{W}^g \\mathbf{x}_t + \\mathbf{U}^g \\mathbf{h}_{t-1} + \\mathbf{b}^g) \\\\\n",
    "\\mathbf{c}_t &= \\mathbf{f}_t \\circ \\mathbf{c}_{t-1} + \\mathbf{i}_t \\circ \\mathbf{g}_t \\\\\n",
    "\\mathbf{h}_t &= \\mathbf{o}_t \\circ \\tanh(\\mathbf{c}_t)\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# implement both lstm cells, direct and highway input\n",
    "lstm_inputs = hwy_outputs\n",
    "M = 600\n",
    "\n",
    "lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(M)\n",
    "\n",
    "lstm_outputs = list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output at time $t$ is achieved by taking the softmax after an affine transformation to the hidden output at time $t$, $\\mathbf{h}_t$. This creates a probability distribution over all possible words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\Pr(w_{t+1} = j | w_{1:t}) = \\frac{\\exp(\\mathbf{h}_t \\cdot \\mathbf{p}^j + q^j)}{\\sum_{j' \\in \\mathcal{V}} \\exp(\\mathbf{h}_t \\cdot \\mathbf{p}^{j'} + q^{j'})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $\\mathbf{p}^j$ is the $j$-th column of $\\mathbf{P} \\in \\mathbb{R}^{m \\times \\vert \\mathcal{V} \\vert}$, an output embedding matrix and $q^j$ is a bias term. Here $\\mathcal{V}$ is simply our vocabulary of words. An lstm taking direct input from the cNN and input from the highway network are both implemented below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# placeholder for true outputs\n",
    "# embedding to create prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models are trained through truncated backpropagation up to 35 time steps. The models are rated by perplexity, the exponent of the averaged negative log likelihood of seeing a sequence of words. These are defined below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "NLL &= -\\sum_{t=1}^T \\log \\Pr(w_t | w_{1:t-1}) \\\\\n",
    "PPL &= \\exp(\\frac{NLL}{T})\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning rate is initially set to 1.0 and is halved if the perplexity is not decreased by 1.0 per training epoch. In addition, the model is regularized using dropout and the gradient is renormalized so that its $L_2$ norm is less than or equal to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# implement training and loss function\n",
    "# implement checks for adjusting training rate and gradient normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
